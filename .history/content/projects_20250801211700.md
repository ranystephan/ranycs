---
title: "Projects"
---

## Research Projects

### Two-Level Learning-Augmented Caching for Long-Context LLM Inference
*2024*

- **Research Paper**: [Two-Level Learning-Augmented Caching for Long-Context LLM Inference: A Beyond Worst-Case Analysis](/cs264project_research_final.pdf)
- **Authors**: Rany Stephan (Stanford University)
- **Abstract**: Large Language Models (LLMs) with extended context windows face a critical challenge: recent empirical studies show accuracy degradation in multi-turn conversations compared to single-turn settings, even within the model's context limit. We formalize this long-chat degradation as a hierarchical online caching problem and introduce LATEM (Learning-Augmented Two-level Marking), a learning-augmented algorithm that manages both message-level and token-level caches. We prove that LATEM achieves consistency ratio 2 + O(η/OPT) when predictions have error η, and O(log km + log k′t/ε)-robustness in the worst case, where km is the message cache size, k′t = ⌊kt/km⌋ is the effective token cache per message, and ε is the trust parameter. Our analysis extends the learning-augmented framework to hierarchical caches with interdependent eviction decisions, offering, to our knowledge, the first formal consistency–robustness guarantees for hierarchical KV-cache management in LLMs.

## Projects & Competitions

### Winner, Murex Best Development Project Award ($3,000)
*June 2023*

- Solely architected and developed **NeuralFin**, a full-stack financial analysis platform using Python (Django, scikit-learn) for predictive stock modeling and sentiment analysis.

### University Representative, Refinitiv Portfolio Management Competition
*Oct 2023*

- Implemented an optimized asset allocation model in **Python**, using **Markowitz's Efficient Frontier** and **Conditional Value at Risk (CVaR)** to maximize risk-adjusted returns.
- Achieved top-quartile performance managing a $1M mock portfolio against international university teams.
